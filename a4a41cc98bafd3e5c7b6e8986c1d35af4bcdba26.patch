From a4a41cc98bafd3e5c7b6e8986c1d35af4bcdba26 Mon Sep 17 00:00:00 2001
From: Andrew Gallatin <gallatin@gmail.com>
Date: Wed, 21 Feb 2018 12:01:58 -0500
Subject: [PATCH] Initial import of kernel TLS from Netflix

This is the kernel side of the Netflix kernel TLS system.
Note that this is based on the original work of scottl, and
rrs, with contributions from jtl and glebius.  It was re-written
by gallatin to use the same not-ready mbuf mechanisms for
queuing as sendfile.

It requires a backend crypto module to perform the actual
encryption.

It requres hooks in userspace to enable TLS, which will be
coming in a separate commit.

Sponsored by: Netflix
---
 sys/amd64/conf/NOTES        |    3 +
 sys/conf/files.amd64        |    4 +
 sys/conf/options.amd64      |    3 +
 sys/kern/kern_sendfile.c    |   79 ++-
 sys/kern/uipc_sockbuf.c     |   12 +
 sys/kern/uipc_sockbuf_tls.c | 1002 +++++++++++++++++++++++++++++++++++
 sys/kern/uipc_socket.c      |   92 +++-
 sys/netinet/tcp.h           |    6 +-
 sys/netinet/tcp_usrreq.c    |   12 +
 sys/sys/sockbuf.h           |    4 +
 sys/sys/sockbuf_tls.h       |  249 +++++++++
 11 files changed, 1442 insertions(+), 24 deletions(-)
 create mode 100644 sys/kern/uipc_sockbuf_tls.c
 create mode 100644 sys/sys/sockbuf_tls.h

diff --git a/sys/amd64/conf/NOTES b/sys/amd64/conf/NOTES
index 526ef1553ae19..c781d3375be79 100644
--- a/sys/amd64/conf/NOTES
+++ b/sys/amd64/conf/NOTES
@@ -675,3 +675,6 @@ options 	VM_KMEM_SIZE_SCALE
 # Enable NDIS binary driver support
 options 	NDISAPI
 device		ndis
+
+# Kernel TLS
+options		KERN_TLS
diff --git a/sys/conf/files.amd64 b/sys/conf/files.amd64
index 4c5537698eb6a..7ca839c846338 100644
--- a/sys/conf/files.amd64
+++ b/sys/conf/files.amd64
@@ -724,3 +724,7 @@ x86/xen/xenpv.c			optional	xenhvm
 x86/xen/xen_nexus.c		optional	xenhvm
 x86/xen/xen_msi.c		optional	xenhvm
 x86/xen/xen_pci_bus.c		optional	xenhvm
+#
+# Kernel TLS support
+#
+kern/uipc_sockbuf_tls.c		optional	kern_tls
diff --git a/sys/conf/options.amd64 b/sys/conf/options.amd64
index 441f191003f90..a422fa4165ad8 100644
--- a/sys/conf/options.amd64
+++ b/sys/conf/options.amd64
@@ -68,3 +68,6 @@ ISCI_LOGGING	opt_isci.h
 
 # EFI Runtime services support
 EFIRT			opt_efirt.h
+
+# accelerated TLS sockets
+KERN_TLS		opt_kern_tls.h
diff --git a/sys/kern/kern_sendfile.c b/sys/kern/kern_sendfile.c
index 0b74c3be3a2c3..57e3ee1eb5247 100644
--- a/sys/kern/kern_sendfile.c
+++ b/sys/kern/kern_sendfile.c
@@ -50,6 +50,7 @@ __FBSDID("$FreeBSD$");
 #include <sys/sf_buf.h>
 #include <sys/socket.h>
 #include <sys/socketvar.h>
+#include <sys/sockbuf_tls.h>
 #include <sys/syscallsubr.h>
 #include <sys/sysctl.h>
 #include <sys/vnode.h>
@@ -340,12 +341,28 @@ sendfile_iodone(void *arg, vm_page_t *pg, int count, int error)
 
 		m = sfio->m;
 		mb_free_notready(m, sfio->npages);
-	} else
-		(void )(so->so_proto->pr_usrreqs->pru_ready)(so, sfio->m,
-		    sfio->npages);
+	} else {
+		if (so->so_snd.sb_tls_flags & SB_TLS_ACTIVE) {
+			/*
+			 * I/O operation is complete, but we still
+			 * need to encrypt.  We cannot do this in the
+			 * interrupt thread of the disk controller, so
+			 * forward the mbufs to a different thread.
+			 *
+			 * Note that the socket was referenced by
+			 * sendfile, and we're inheriting that ref,
+			 * so we do not need to do an soref() here.
+			 */
+			sbtls_enqueue(sfio->m, so, sfio->npages);
+			goto out_with_ref;
+		} else
+			(void )(so->so_proto->pr_usrreqs->pru_ready)(so,
+			    sfio->m, sfio->npages);
+	}
 
 	SOCK_LOCK(so);
 	sorele(so);
+out_with_ref:
 	CURVNET_RESTORE();
 	free(sfio, M_TEMP);
 }
@@ -585,9 +602,10 @@ vn_sendfile(struct file *fp, int sockfd, struct uio *hdr_uio,
 	off_t off, sbytes, rem, obj_size;
 	struct mbuf_ext_pgs *ext_pgs;
 	struct mbuf *m0;
+	struct sbtls_info *tls;
 	int use_ext_pgs = 0;
 	int error, softerr, bsize, hdrlen;
-	int ext_pgs_idx;
+	int ext_pgs_idx, max_pgs, tls_enq_cnt;
 
 	obj = NULL;
 	so = NULL;
@@ -628,6 +646,10 @@ vn_sendfile(struct file *fp, int sockfd, struct uio *hdr_uio,
 	 * we implement that, but possibly shouldn't.
 	 */
 	(void)sblock(&so->so_snd, SBL_WAIT | SBL_NOINTR);
+	if (so->so_snd.sb_tls_flags & SB_TLS_ACTIVE)
+		tls = so->so_snd.sb_tls_info;
+	else
+		tls = NULL;
 
 	/*
 	 * Loop through the pages of the file, starting with the requested
@@ -721,7 +743,14 @@ vn_sendfile(struct file *fp, int sockfd, struct uio *hdr_uio,
 		if (hdr_uio != NULL && hdr_uio->uio_resid > 0) {
 			hdr_uio->uio_td = td;
 			hdr_uio->uio_rw = UIO_WRITE;
-			mh = m_uiotombuf(hdr_uio, M_WAITOK, space, 0, 0);
+			if (tls == NULL) {
+				mh = m_uiotombuf(hdr_uio, M_WAITOK,
+				    space, 0, 0);
+			} else {
+				mh = m_uiotombuf(hdr_uio, M_WAITOK,
+				    space, tls->sb_params.sb_maxlen,
+				    M_NOMAP);
+			}
 			hdrlen = m_length(mh, &mhtail);
 			space -= hdrlen;
 			/*
@@ -755,6 +784,16 @@ vn_sendfile(struct file *fp, int sockfd, struct uio *hdr_uio,
 
 		if (space > rem)
 			space = rem;
+		else if (space > PAGE_SIZE) {
+			/*
+			 * Keep TLS chunked at a page boundary, when possible.
+			 */
+			if (off & PAGE_MASK)
+				space -= (PAGE_SIZE - (off & PAGE_MASK));
+			space = trunc_page(space);
+			if (off & PAGE_MASK)
+				space += (PAGE_SIZE - (off & PAGE_MASK));
+		}
 
 		npages = howmany(space + (off & PAGE_MASK), PAGE_SIZE);
 
@@ -794,11 +833,16 @@ vn_sendfile(struct file *fp, int sockfd, struct uio *hdr_uio,
 		pa = sfio->pa;
 
 		if ((mb_use_ext_pgs &&
-			so->so_proto->pr_protocol == IPPROTO_TCP)) {
+			so->so_proto->pr_protocol == IPPROTO_TCP) ||
+		    tls != NULL) {
 			/* cache state in a local, to avoid locks */
 			use_ext_pgs = 1;
+			if (tls != NULL)
+				max_pgs = num_pages(tls->sb_params.sb_maxlen);
+			else
+				max_pgs = MBUF_PEXT_MAX_PGS;
 			/* start at last index, to wrap to first */
-			ext_pgs_idx = MBUF_PEXT_MAX_PGS - 1;
+			ext_pgs_idx = max_pgs - 1;
 			m0 = NULL; /* -Wsometimes-uninitialized */
 		}
 
@@ -820,7 +864,7 @@ vn_sendfile(struct file *fp, int sockfd, struct uio *hdr_uio,
 				off_t xfs;
 
 				ext_pgs_idx++;
-				if (ext_pgs_idx == MBUF_PEXT_MAX_PGS) {
+				if (ext_pgs_idx == max_pgs) {
 					m0 = mb_alloc_ext_pgs(M_WAITOK, false,
 					    sendfile_free_mext_pg);
 
@@ -834,7 +878,7 @@ vn_sendfile(struct file *fp, int sockfd, struct uio *hdr_uio,
 						 * last page.
 						 */
 						if ((i + min(npages - i,
-						    MBUF_PEXT_MAX_PGS) == npages) &&
+						    max_pgs) == npages) &&
 						    ((off + space) & PAGE_MASK) &&
 						    (rem > space || rhpages > 0))
 							m0->m_ext.ext_flags |=
@@ -974,6 +1018,12 @@ vn_sendfile(struct file *fp, int sockfd, struct uio *hdr_uio,
 		    __func__, m_length(m, NULL), space, hdrlen));
 
 		CURVNET_SET(so->so_vnet);
+		if (tls != NULL) {
+			error = sbtls_frame(&m, tls, &tls_enq_cnt,
+			    TLS_RLTYPE_APP);
+			if (error != 0)
+				goto done;
+		}
 		if (nios == 0) {
 			/*
 			 * If sendfile_swapin() didn't initiate any I/Os,
@@ -982,8 +1032,15 @@ vn_sendfile(struct file *fp, int sockfd, struct uio *hdr_uio,
 			 * PRUS_NOTREADY flag.
 			 */
 			free(sfio, M_TEMP);
-			error = (*so->so_proto->pr_usrreqs->pru_send)
-			    (so, 0, m, NULL, NULL, td);
+			if (tls != NULL) {
+				soref(so);
+				error = (*so->so_proto->pr_usrreqs->pru_send)
+				    (so, PRUS_NOTREADY, m, NULL, NULL, td);
+				sbtls_enqueue(m, so, tls_enq_cnt);
+			} else {
+				error = (*so->so_proto->pr_usrreqs->pru_send)
+				    (so, 0, m, NULL, NULL, td);
+			}
 		} else {
 			sfio->npages = npages;
 			soref(so);
diff --git a/sys/kern/uipc_sockbuf.c b/sys/kern/uipc_sockbuf.c
index 2e0c871d36de1..bb702f8f21345 100644
--- a/sys/kern/uipc_sockbuf.c
+++ b/sys/kern/uipc_sockbuf.c
@@ -49,6 +49,7 @@ __FBSDID("$FreeBSD$");
 #include <sys/signalvar.h>
 #include <sys/socket.h>
 #include <sys/socketvar.h>
+#include <sys/sockbuf_tls.h>
 #include <sys/sx.h>
 #include <sys/sysctl.h>
 
@@ -581,8 +582,16 @@ sbrelease(struct sockbuf *sb, struct socket *so)
 void
 sbdestroy(struct sockbuf *sb, struct socket *so)
 {
+	int locked = 0;
 
+	if (SOCKBUF_OWNED(sb) == 0) {
+		SOCKBUF_LOCK(sb);
+		locked = 1;
+	}
 	sbrelease_internal(sb, so);
+	sbtlsdestroy(sb);
+	if (locked)
+		SOCKBUF_UNLOCK(sb);
 }
 
 /*
@@ -746,6 +755,9 @@ sbappendstream_locked(struct sockbuf *sb, struct mbuf *m, int flags)
 
 	SBLASTMBUFCHK(sb);
 
+	if ((sb->sb_tls_flags & SB_TLS_ACTIVE) != 0)
+		sbtls_seq(sb, m);
+
 	/* Remove all packet headers and mbuf tags to get a pure data chain. */
 	m_demote(m, 1, flags & PRUS_NOTREADY ? M_NOTREADY : 0);
 
diff --git a/sys/kern/uipc_sockbuf_tls.c b/sys/kern/uipc_sockbuf_tls.c
new file mode 100644
index 0000000000000..1719e3daaf19d
--- /dev/null
+++ b/sys/kern/uipc_sockbuf_tls.c
@@ -0,0 +1,1002 @@
+/*-
+ * SPDX-License-Identifier: BSD-2-Clause
+ *
+ * Copyright (c) 2014-2018  Netflix Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ *
+ */
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/kernel.h>
+#include <sys/lock.h>
+#include <sys/mbuf.h>
+#include <sys/mutex.h>
+#include <sys/rmlock.h>
+#include <sys/proc.h>
+#include <sys/protosw.h>
+#include <sys/refcount.h>
+#include <sys/socket.h>
+#include <sys/socketvar.h>
+#include <sys/sockbuf_tls.h>
+#include <sys/sysctl.h>
+#include <sys/kthread.h>
+#include <machine/fpu.h>
+#include <machine/vmparam.h>
+#include <netinet/in.h>
+#include <netinet/in_pcb.h>
+#include <sys/smp.h>
+#include <sys/uio.h>
+#include <vm/vm.h>
+#include <vm/vm_pageout.h>
+#include <sys/vmmeter.h>
+#include <vm/vm_page.h>
+
+#include "opt_rss.h"
+#ifdef RSS
+#include <net/netisr.h>
+#include <net/rss_config.h>
+static int sbtls_bind_threads = 1;
+#else
+static int sbtls_bind_threads;
+#endif
+
+
+
+#include <opencrypto/xform.h>
+#ifndef EVP_AEAD_AES_GCM_TAG_LEN
+#define EVP_AEAD_AES_GCM_TAG_LEN 16
+#endif
+
+static int sbtls_offload_disable = 1;
+static struct proc *sbtls_proc = NULL;
+static struct rmlock sbtls_backend_lock;
+
+int sbtls_allow_unload;
+LIST_HEAD(sbtls_backend_head, sbtls_crypto_backend) sbtls_backend_head = 
+    LIST_HEAD_INITIALIZER(sbtls_backend_head);
+static uma_zone_t zone_tlssock;
+static int sbtls_number_threads;
+
+SYSCTL_DECL(_kern_ipc);
+
+SYSCTL_NODE(_kern_ipc, OID_AUTO, tls, CTLFLAG_RW, 0,
+    "TLS offload IPC calls");
+SYSCTL_NODE(_kern_ipc_tls, OID_AUTO, stats, CTLFLAG_RW, 0,
+    "TLS offload stats");
+SYSCTL_NODE(_kern_ipc_tls, OID_AUTO, counters, CTLFLAG_RW, 0,
+    "TLS offload counters");
+
+SYSCTL_INT(_kern_ipc_tls, OID_AUTO, allow_unload, CTLFLAG_RDTUN,
+    &sbtls_allow_unload, 0, "Allow backend crypto modules to unload");
+
+SYSCTL_INT(_kern_ipc_tls, OID_AUTO, bind_threads, CTLFLAG_RDTUN,
+    &sbtls_bind_threads, 0,
+    "Bind crypto threads to cores or domains at boot");
+
+SYSCTL_INT(_kern_ipc_tls_stats, OID_AUTO, threads, CTLFLAG_RD,
+    &sbtls_number_threads, 0,
+    "Number of TLS threads in thread-pool");
+
+SYSCTL_UINT(_kern_ipc_tls, OID_AUTO, disable, CTLFLAG_RW,
+    &sbtls_offload_disable, 0,
+    "Disable Support of KERNEL TLS offload");
+
+static int sbtls_cbc_disable;
+
+SYSCTL_UINT(_kern_ipc_tls, OID_AUTO, cbc_disable, CTLFLAG_RW,
+    &sbtls_cbc_disable, 1,
+    "Disable Support of AES CBC crypto");
+
+static counter_u64_t sbtls_tasks_active;
+
+SYSCTL_COUNTER_U64(_kern_ipc_tls, OID_AUTO, tasks_active, CTLFLAG_RD,
+    &sbtls_tasks_active, "Number of active tasks");
+
+static counter_u64_t sbtls_cnt_on;
+
+SYSCTL_COUNTER_U64(_kern_ipc_tls_stats, OID_AUTO, so_inqueue, CTLFLAG_RD,
+    &sbtls_cnt_on, "Number of sockets in queue to tasks");
+
+/* Sysctl counters */
+static counter_u64_t sbtls_offload_total;
+static counter_u64_t sbtls_offload_enable_calls;
+static counter_u64_t sbtls_offload_active;
+static counter_u64_t sbtls_offload_failed_crypto;
+
+SYSCTL_COUNTER_U64(_kern_ipc_tls_counters, OID_AUTO, offload_total,
+    CTLFLAG_RD, &sbtls_offload_total,
+    "Total succesful TLS setups (parameters set)");
+SYSCTL_COUNTER_U64(_kern_ipc_tls_counters, OID_AUTO, enable_calls,
+    CTLFLAG_RD, &sbtls_offload_enable_calls,
+    "Total number of TLS enable calls made");
+SYSCTL_COUNTER_U64(_kern_ipc_tls_stats, OID_AUTO, active, CTLFLAG_RD,
+    &sbtls_offload_active, "Total Active TLS sessions");
+SYSCTL_COUNTER_U64(_kern_ipc_tls_stats, OID_AUTO, failed_crypto, CTLFLAG_RD,
+    &sbtls_offload_failed_crypto, "TotalTLS crypto failures");
+
+MALLOC_DEFINE(M_TLSSOBUF, "tls_sobuf", "TLS Socket Buffer");
+
+
+struct sbtls_wq {
+	struct mtx			mtx;
+	STAILQ_HEAD(, mbuf_ext_pgs)	head;
+	int				running;
+} __aligned(CACHE_LINE_SIZE);
+
+
+static struct sbtls_wq *sbtls_wq;
+static void sbtls_work_thread(void *ctx);
+
+
+
+int
+sbtls_crypto_backend_register(struct sbtls_crypto_backend *be)
+{
+	struct sbtls_crypto_backend *curr_be, *tmp;
+
+	if (be->api_version != SBTLS_API_VERSION) {
+		printf("API version mismatch (%d vs %d) for %s\n",
+		    be->api_version, SBTLS_API_VERSION,
+		    be->name);
+		return EINVAL;
+	}
+
+	rm_wlock(&sbtls_backend_lock);
+	printf("Registering crypto method: %s with prio %d\n",
+	       be->name, be->prio);
+	if (LIST_EMPTY(&sbtls_backend_head)) {
+		LIST_INSERT_HEAD(&sbtls_backend_head, be, next);
+	} else {
+		LIST_FOREACH_SAFE(curr_be, &sbtls_backend_head, next, tmp) {
+			if (curr_be->prio < be->prio) {
+				LIST_INSERT_BEFORE(curr_be, be, next);
+				break;
+			}
+			if (LIST_NEXT(curr_be, next) == NULL)
+				LIST_INSERT_AFTER(curr_be, be, next);
+		}
+	}
+	rm_wunlock(&sbtls_backend_lock);
+	return 0;
+}
+
+int
+sbtls_crypto_backend_deregister(struct sbtls_crypto_backend *be)
+{
+	int err = 0;
+
+	if (!sbtls_allow_unload) {
+		printf("deregistering crypto method %s is not supported\n",
+		    be->name);
+		return (EBUSY);
+	}
+
+	rm_wlock(&sbtls_backend_lock);
+	if (be->use_count) {
+		err = EBUSY;
+	} else {
+		LIST_REMOVE(be, next);
+	}
+	rm_wunlock(&sbtls_backend_lock);
+	return (err);
+}
+
+static uint16_t
+sbtls_get_cpu(struct socket *so)
+{
+	uint16_t cpuid;
+	struct inpcb *inp;
+
+	inp = sotoinpcb(so);
+#ifdef	RSS
+	cpuid = rss_hash2cpuid(inp->inp_flowid, inp->inp_flowtype);
+	if (cpuid != NETISR_CPUID_NONE)
+		return (cpuid);
+#endif
+	/* 
+	 * Just use the flowid to shard connections in a repeatable fashion.
+	 * Note that some crypto backends rely on the serialization provided by
+	 * having the same connection use the same queue.
+	 */
+	cpuid = inp->inp_flowid % (sbtls_number_threads);
+	return (cpuid);
+
+}
+
+
+static void
+sbtls_init(void *st __unused)
+{
+	int domain, error, i, j;
+	cpuset_t mask;
+	struct pcpu *pc;
+	struct thread **sbtls_ctx;
+
+	/*
+	 * Initialize the task's to run the TLS work. We create a task
+	 * per cpu. Each task is separate with a separate queue.
+	 */
+
+	/* Init counters */
+	sbtls_tasks_active = counter_u64_alloc(M_WAITOK);
+	sbtls_cnt_on = counter_u64_alloc(M_WAITOK);
+	sbtls_offload_total = counter_u64_alloc(M_WAITOK);
+	sbtls_offload_enable_calls = counter_u64_alloc(M_WAITOK);
+	sbtls_offload_active = counter_u64_alloc(M_WAITOK);
+	sbtls_offload_failed_crypto = counter_u64_alloc(M_WAITOK);
+
+	rm_init(&sbtls_backend_lock, "sbtls crypto backend lock");
+
+	sbtls_number_threads = mp_ncpus;
+
+	sbtls_wq = malloc(sizeof(*sbtls_wq) * mp_ncpus, M_TLSSOBUF,
+	    M_WAITOK | M_ZERO);
+
+	zone_tlssock = uma_zcreate("sbtls_sockent",
+	    sizeof(struct sbtls_info),
+	    NULL, NULL, NULL, NULL, UMA_ALIGN_CACHE, 0);
+
+	sbtls_ctx = malloc((sizeof(struct thread *) * mp_ncpus),
+	    M_TLSSOBUF, M_WAITOK | M_ZERO);
+
+	error = kproc_kthread_add(sbtls_work_thread, &sbtls_wq[0],
+	    &sbtls_proc, &sbtls_ctx[0],  0, 0, "TLS_proc", "tls_thr_0");
+
+	if (error)
+		panic("Can't start TLS_proc err:%d", error);
+
+	for (i = 1; i < mp_ncpus; i++) {
+		error = kthread_add(sbtls_work_thread, &sbtls_wq[i],
+		    sbtls_proc, &sbtls_ctx[i], 0, 0, "tls_thr_%d", i);
+		if (error)
+			panic("Can't add TLS_thread %d err:%d", i, error);
+	}
+	/*
+	 * Bind threads to cores.  If sbtls_bind_threads is > 1, then
+	 * we bind to the NUMA domain.
+	 */
+	if (sbtls_bind_threads) {
+		CPU_FOREACH(i) {
+			CPU_ZERO(&mask);
+			if (sbtls_bind_threads > 1) {
+				pc = pcpu_find(i);
+				domain = pc->pc_domain;
+				CPU_FOREACH(j) {
+					pc = pcpu_find(j);
+					if (pc->pc_domain == domain)
+						CPU_SET(j, &mask);
+				}
+			} else {
+				CPU_SET(i, &mask);
+			}
+			error |= cpuset_setthread(sbtls_ctx[i]->td_tid,
+			    &mask);
+		}
+		if (error) {
+			printf("unable to bind crypto threads\n");
+		}
+	}
+	printf("SBTLS: Initialized %d threads\n", sbtls_number_threads);
+}
+
+SYSINIT(sbtls, SI_SUB_SMP + 1, SI_ORDER_ANY, sbtls_init, NULL);
+
+
+
+struct sbtls_info *
+sbtls_init_sb_tls(struct socket *so, struct tls_so_enable *en, size_t size)
+{
+	static int warn_once = 0;
+	struct sbtls_info *tls;
+	void *cipher;
+
+	tls = uma_zalloc(zone_tlssock, M_NOWAIT | M_ZERO);
+	if (tls == NULL)
+		return (NULL);
+
+	cipher = malloc(size, M_TLSSOBUF, M_NOWAIT | M_ZERO);
+	if (cipher == NULL) {
+		uma_zfree(zone_tlssock, tls);
+		return (NULL);
+	}
+	tls->cipher = cipher;
+	so->so_snd.sb_tls_info = tls;
+	so->so_snd.sb_tls_flags =  SB_TLS_SEND_SIDE | SB_TLS_CRY_INI,
+
+
+	/* cache cpu index */
+	tls->sb_tsk_instance = sbtls_get_cpu(so);
+
+	tls->sb_params.sb_tls_vmajor = en->tls_vmajor;
+	tls->sb_params.sb_tls_vminor = en->tls_vminor;
+
+	/* Determine max size */
+	if (tls->sb_params.sb_tls_vmajor == TLS_MAJOR_VER_ONE) {
+		/* 
+		 *  note that 1.3 was supposed to go to 64K, but that 
+		 * was shot down
+		 */
+		tls->sb_params.sb_maxlen = TLS_MAX_MSG_SIZE_V10_2;
+
+	} else {
+		/*
+		 * Unknown play it safe and frag at V1.0-2 size
+		 * (16k).
+		 */
+		if (warn_once == 0) {
+			printf("Warning saw TLS version major:%d -- unknown size limited to 16k\n",
+			    tls->sb_params.sb_tls_vmajor);
+			warn_once = 1;
+		}
+		tls->sb_params.sb_maxlen = TLS_MAX_MSG_SIZE_V10_2;
+	}
+	counter_u64_add(sbtls_offload_active, 1);
+	return (tls);
+}
+
+static void
+sbtls_cleanup(struct sbtls_info *tls)
+{
+	void *cipher;
+
+	if (NULL != (cipher = tls->cipher)) {
+		counter_u64_add(sbtls_offload_active, -1);
+		tls->cipher = NULL;
+		if (tls->be != NULL && tls->be->clean_cipher != NULL)
+			tls->be->clean_cipher(tls, cipher);
+		free(cipher, M_TLSSOBUF);
+	}
+	if (tls->sb_params.hmac_key) {
+		free(tls->sb_params.hmac_key, M_TLSSOBUF);
+		tls->sb_params.hmac_key = NULL;
+		tls->sb_params.hmac_key_len = 0;
+	}
+	if (tls->sb_params.crypt) {
+		free(tls->sb_params.crypt, M_TLSSOBUF);
+		tls->sb_params.crypt = NULL;
+		tls->sb_params.crypt_key_len = 0;
+	}
+	if (tls->sb_params.iv) {
+		free(tls->sb_params.iv, M_TLSSOBUF);
+		tls->sb_params.iv = NULL;
+		tls->sb_params.iv_len = 0;
+	}
+}
+
+int
+sbtls_crypt_tls_enable(struct socket *so, struct tls_so_enable *en)
+{
+	struct rm_priotracker prio;
+	struct sbtls_crypto_backend *be;
+	struct sbtls_info *tls;
+	int error = 0;
+
+
+	if (sbtls_offload_disable) {
+		return (ENOTSUP);
+	}
+	counter_u64_add(sbtls_offload_enable_calls, 1);
+	if (so->so_proto->pr_protocol != IPPROTO_TCP) {
+		/* We can only support TCP for now */
+		return (EINVAL);
+	}
+
+	if (so->so_snd.sb_tls_info != NULL) {
+		/* Already setup, you get to do it once per socket. */
+		return (EALREADY);
+	}
+
+	if (en->crypt_algorithm == CRYPTO_AES_CBC && sbtls_cbc_disable)
+		return (ENOTSUP);
+
+	/* TLS requires ext pgs */
+	if (mb_use_ext_pgs == 0)
+		return (ENXIO);
+
+	/*
+	 * Now lets find the algorithms if possible. The idea here is we
+	 * prioritize what to use. 1) Hardware, if we have an offload card
+	 * use it. 2) INTEL ISA lib which is faster than BoringSSL. 3)
+	 * Finally if nothing else we try boring SSL.
+	 * 
+	 * As noted in the sbtls_try_hardware comments, that is more historic
+	 * and needs to be re-written with async in mind as well as folding
+	 * the SID/et.al. into its own structure. But that will come when we
+	 * intergrate the intel QAT card.
+	 */
+
+	if (sbtls_allow_unload)
+		rm_rlock(&sbtls_backend_lock, &prio);
+
+	LIST_FOREACH(be, &sbtls_backend_head, next) {
+		if (be->try(so, en, &error) == 0) {
+			so->so_snd.sb_tls_info->be = be;
+			break;
+		}
+	}
+	if (sbtls_allow_unload) {
+		if (so->so_snd.sb_tls_info != NULL)
+			be->use_count++;
+		rm_runlock(&sbtls_backend_lock, &prio);
+	}
+	if (so->so_snd.sb_tls_info == NULL)
+		return (ENOTSUP);
+
+	tls = so->so_snd.sb_tls_info;
+
+	/* Now lets get in the keys and such */
+	if (en->hmac_key_len && en->hmac_key &&
+	    (en->hmac_key_len <= TLS_MAX_PARAM_SIZE)) {
+		tls->sb_params.hmac_key_len = en->hmac_key_len;
+		tls->sb_params.hmac_key = malloc(en->hmac_key_len,
+		    M_TLSSOBUF, M_NOWAIT);
+		if (tls->sb_params.hmac_key == NULL) {
+			error = ENOMEM;
+			goto out;
+		}
+		error = copyin_nofault(en->hmac_key, tls->sb_params.hmac_key,
+		    en->hmac_key_len);
+		if (error)
+			goto out;
+	}
+	if (en->crypt_key_len && en->crypt &&
+	    (en->crypt_key_len <= TLS_MAX_PARAM_SIZE)) {
+		tls->sb_params.crypt_key_len = en->crypt_key_len;
+		tls->sb_params.crypt = malloc(en->crypt_key_len,
+		    M_TLSSOBUF, M_NOWAIT);
+		if (tls->sb_params.crypt == NULL) {
+			error = ENOMEM;
+			goto out;
+		}
+		error = copyin_nofault(en->crypt, tls->sb_params.crypt,
+		    en->crypt_key_len);
+		if (error)
+			goto out;
+	}
+	/*
+	 * We allow these to be set as a number to indicate how many random
+	 * bytes to send if iv is present, then its for an AEAD fixed part
+	 * nonce.
+	 */
+	if (en->iv_len && en->iv &&
+	    (en->iv_len <= TLS_MAX_PARAM_SIZE)) {
+		tls->sb_params.iv = malloc(en->iv_len,
+		    M_TLSSOBUF, M_NOWAIT);
+		tls->sb_params.iv_len = en->iv_len;
+		if (tls->sb_params.iv == NULL) {
+			error = ENOMEM;
+			goto out;
+		}
+		error = copyin_nofault(en->iv, tls->sb_params.iv,
+		    en->iv_len);
+		if (error)
+			goto out;
+	}
+	tls->be->setup_cipher(tls, &error);
+	if (error)
+		goto out;
+
+	if (tls->sb_params.sb_tls_hlen > MBUF_PEXT_HDR_LEN ||
+	    tls->sb_params.sb_tls_tlen > MBUF_PEXT_TRAIL_LEN){
+		static int warned = 0;
+		if (!warned) {
+			warned = 1;
+			printf(" %s: %p exceeded hdr/trl len (%d/%d)\n",
+			    be->name, tls->sb_tls_crypt,
+			    tls->sb_params.sb_tls_hlen,
+			    tls->sb_params.sb_tls_tlen);
+		}
+		error = ENXIO;
+		goto out;
+	}
+
+
+	so->so_snd.sb_tls_flags &= (~SB_TLS_CRY_INI);
+	so->so_snd.sb_tls_flags |= SB_TLS_ACTIVE;
+
+	counter_u64_add(sbtls_offload_total, 1);
+
+	return (0);
+
+out:
+	sbtlsdestroy(&so->so_snd);
+	return (error);
+}
+
+void
+sbtls_free_tls(struct sbtls_info *tls)
+{
+	struct rm_priotracker prio;
+
+
+	if (tls->be != NULL && sbtls_allow_unload) {
+		rm_rlock(&sbtls_backend_lock, &prio);
+		tls->be->use_count--;
+		rm_runlock(&sbtls_backend_lock, &prio);
+	}
+	uma_zfree(zone_tlssock, tls);
+}
+
+void
+sbtlsdestroy(struct sockbuf *sb)
+{
+	struct sbtls_info *tls;
+
+
+	tls = sb->sb_tls_info;
+	sb->sb_tls_info = NULL;
+	sb->sb_tls_flags = 0;
+	if (tls) {
+		sbtls_cleanup(tls);
+		sbtls_free_tls(tls);
+	}
+}
+
+void
+sbtls_seq(struct sockbuf *sb, struct mbuf *m)
+{
+	struct mbuf_ext_pgs *pgs;
+
+
+	for (; m != NULL; m = m->m_next) {
+		if (0 == (m->m_flags & M_NOMAP))
+			panic("tls with normal mbuf\n");
+
+		pgs = (void *)m->m_ext.ext_buf;
+		pgs->seqno = sb->sb_tls_seqno;
+		sb->sb_tls_seqno++;
+	}
+}
+
+int
+sbtls_frame(struct mbuf **top, struct sbtls_info *tls, int *enq_cnt,
+    uint8_t record_type)
+{
+	struct tls_record_layer *tlshdr;
+	struct mbuf *m;
+	struct mbuf_ext_pgs *pgs;
+	uint16_t tls_len;
+	int maxlen;
+
+
+	maxlen = tls->sb_params.sb_maxlen;
+	*enq_cnt = 0;
+	for (m = *top; m != NULL; m = m->m_next) {
+		/*
+		 * We expect whoever constructed the chain
+		 * to have put no more than maxlen in each
+		 * mbuf.
+		 */
+		if (m->m_len > maxlen || m->m_len == 0)
+			return (EINVAL);
+
+
+		tls_len = m->m_len;
+
+		/*
+		 * we don't yet support inserting framing into
+		 * normal mbuf chains.  For now, just panic if
+		 * we see one.  Eventually, we'll be sticking
+		 * a tls hdr mbuf at the start, which is why
+		 * top is a pointer to a pointer
+		 */
+		KASSERT(((m->m_flags & M_NOMAP) == M_NOMAP),
+		    ("Can't Frame %p: not nomap mbuf(top = %p)\n", m, *top));
+
+
+		pgs = (void *)m->m_ext.ext_buf;
+		tlshdr = (void *)pgs->hdr;
+		tlshdr->tls_vmajor =  tls->sb_params.sb_tls_vmajor;
+		tlshdr->tls_vminor =  tls->sb_params.sb_tls_vminor;
+		tlshdr->tls_type = record_type;
+		tlshdr->tls_length = htons(tls_len);
+
+		pgs->hdr_len = tls->sb_params.sb_tls_hlen;
+		pgs->trail_len = tls->sb_params.sb_tls_tlen;
+
+		if (tls->t_type == SBTLS_T_TYPE_BSSL) {
+			int bs, delta;
+
+			/*
+			 * CBC pads messages to a multiple of block
+			 * size.  Try to figure out what the final
+			 * trailer len will be.  Note that the padding
+			 * calculation must include the digest len, as
+			 * it is not always a multiple of the block
+			 * size.  tls->sb_params.sb_tls_tlen is the
+			 * max possible len (padding + digest), so
+			 * what we're doing here is actually removing
+			 * padding.
+			 */
+
+			bs = tls->sb_params.sb_tls_bs;
+			delta = (tls_len + tls->sb_params.sb_tls_tlen) &
+			    (bs - 1);
+			pgs->trail_len -= delta;
+		}
+		m->m_len += pgs->hdr_len + pgs->trail_len;
+
+		/* mark mbuf not-ready, to be cleared when encrypted */
+		m->m_flags |= M_NOTREADY;
+		pgs->nrdy = pgs->npgs;
+		*enq_cnt += pgs->npgs;
+	}
+	return (0);
+}
+
+
+void
+sbtls_enqueue(struct mbuf *m, struct socket *so, int page_count)
+{
+	struct sbtls_info *tls = so->so_snd.sb_tls_info;
+	struct mbuf_ext_pgs *pgs;
+	struct sbtls_wq *wq;
+	int running;
+
+
+	KASSERT(((m->m_flags & (M_NOMAP | M_NOTREADY)) ==
+		(M_NOMAP | M_NOTREADY)),
+	    ("%p not unready & nomap mbuf\n", m));
+
+
+	if (page_count == 0)
+		panic("enq_cnt = 0\n");
+
+	pgs = (void *)m->m_ext.ext_buf;
+	pgs->enc_cnt = page_count;
+	pgs->mbuf = m;
+
+	/* save a pointer to the socket */
+	pgs->so = so;
+
+	wq = &sbtls_wq[tls->sb_tsk_instance];
+	mtx_lock(&wq->mtx);
+	STAILQ_INSERT_TAIL(&wq->head, pgs, stailq);
+	running = wq->running;
+	mtx_unlock(&wq->mtx);
+	if (!running)
+		wakeup(wq);
+	counter_u64_add(sbtls_cnt_on, 1);
+}
+
+static void
+sbtls_boring_fixup(struct sbtls_info *tls, struct sockbuf *sb,
+    struct mbuf *m, struct iovec *dst_iov, int *pgcnt)
+{
+	struct mbuf_ext_pgs *pgs;
+	struct vm_page *pg;
+	struct iovec *iov;
+	int i, pg_delta, tag_delta, len, off;
+
+
+	/*
+	 * Boring CBC will shuffle data around in order to better
+	 * align things.  We need to account for several different
+	 * cases, where data can move into or out of the first or
+	 * last segment.  We do not expect any middle segments to
+	 * be impacted.
+	 *
+	 * Holding the sb lock is not required, when we are only
+	 * changing the internal layout of the mbuf; we are not
+	 * changing its length.  Because the mbuf is marked
+	 * M_NOTREADY, and nothing in the socket buffer code that
+	 * deals with M_NOTREADY can look inside one, changing the
+	 * layout is safe.
+	 *
+	 * The exception is removing pages, which is reflected in
+	 * decrease in ext_size and sb_mbcnt.  This does require
+	 * the lock.
+	 *
+	 * Note that if the session is closed, we avoid fixing up
+	 * the mbuf.  This is because we both don't care what's
+	 * in the mbuf at this point (since it cannot be sent),
+	 * and because sb_free() may have already run and accounted
+	 * for the page we're about to remove when it decremented
+	 * sb_mbcnt.
+	 */
+
+	pgs = (void *)m->m_ext.ext_buf;
+	tag_delta = tls->taglen - pgs->trail_len;
+	pgs->trail_len += tag_delta;
+	off = pgs->first_pg_off;
+	for (i = 0, iov = dst_iov; i < pgs->npgs; i++, iov++) {
+		len = mbuf_ext_pg_len(pgs, i, off);
+		off = 0;
+		pg_delta = iov->iov_len - len;
+		if (pg_delta == 0)
+			continue;
+
+		/* try to fix the mess boring has made */
+
+		if (pgs->npgs == 1 && iov->iov_len == 0) {
+			/*
+			 *  if the only page is removed, then downgrade
+			 * it to a normal mbuf
+			 */
+
+			SOCKBUF_LOCK(sb);
+			if (sb->sb_state & SBS_CANTSENDMORE) {
+				SOCKBUF_UNLOCK(sb);
+				return;
+			}
+			mb_ext_pgs_downgrade(m);
+			sb->sb_mbcnt -= PAGE_SIZE;
+			SOCKBUF_UNLOCK(sb);
+			/*
+			 * must return here, as pgs has been freed
+			 * Note: We must not decease pgcnt!  This
+			 * is required so that sbready will ready the
+			 * plain mbuf.
+			 */
+			return;
+		} else if (i == pgs->npgs - 1) {
+			/*
+			 * Handle changes to the last page
+			 */
+
+			pgs->last_pg_len = iov->iov_len;
+			if (pgs->last_pg_len == 0) {
+				/* last segment entirely removed */
+				SOCKBUF_LOCK(sb);
+				if (sb->sb_state & SBS_CANTSENDMORE) {
+					SOCKBUF_UNLOCK(sb);
+					return;
+				}
+				*pgcnt = *pgcnt + 1;
+				pg = PHYS_TO_VM_PAGE(pgs->pa[i]);
+				pg->flags &= ~PG_ZERO;
+				vm_page_free_toq(pg);
+				vm_wire_sub(1);
+				pgs->last_pg_len = PAGE_SIZE;
+				pgs->npgs -= 1;
+				pgs->nrdy -= 1;
+				/*
+				 * If this is the only page, then its
+				 * length must reflect the 1st page off
+				 */
+				if (pgs->npgs == 1)
+					pgs->last_pg_len -=
+					    pgs->first_pg_off;
+				sb->sb_mbcnt -= PAGE_SIZE;
+				m->m_ext.ext_size -= PAGE_SIZE;
+				SOCKBUF_UNLOCK(sb);
+			}
+		} else {
+			/*
+			 * Handle changes to the first page
+			 */
+
+			if (i != 0)
+				panic("boring removed dat in the middle: %p %p %p %d %d?!?!\n",
+				    m, pgs, dst_iov, pg_delta, i);
+			if (iov->iov_len == 0) {
+				/* first segment entirely removed */
+				SOCKBUF_LOCK(sb);
+				if (sb->sb_state & SBS_CANTSENDMORE) {
+					SOCKBUF_UNLOCK(sb);
+					return;
+				}
+				*pgcnt = *pgcnt + 1;
+				pg = PHYS_TO_VM_PAGE(pgs->pa[i]);
+				pg->flags &= ~PG_ZERO;
+				vm_page_free_toq(pg);
+				vm_wire_sub(1);
+				pgs->first_pg_off = 0;
+				pgs->npgs -= 1;
+				pgs->nrdy -= 1;
+				/* move remainder of pgs down */
+				ovbcopy(&pgs->pa[1], &pgs->pa[0],
+				    pgs->npgs * sizeof(pgs->pa[0]));
+
+				/*
+				 * back up loop index to look at the
+				 * page we just moved into place, else
+				 * it will be skipped over
+				 */
+				i--;
+
+				sb->sb_mbcnt -= PAGE_SIZE;
+				m->m_ext.ext_size -= PAGE_SIZE;
+				SOCKBUF_UNLOCK(sb);
+			} else {
+				/*
+				 * Remove data from the first segment:
+				 *
+				 * We have no way to express the length of
+				 * the first segment except for the offset.
+				 * However, boring reduces the length
+				 * and leaves the offset the same.  The only
+				 * way to recover is to copy the data so
+				 * that it starts at the adjusted offset.
+				 *
+				 * Note: pg_delta is negative, that is why
+				 * it is subtracted.
+				 *
+				 */
+				ovbcopy(iov->iov_base,
+				    (caddr_t)iov->iov_base - pg_delta,
+				    iov->iov_len);
+				pgs->first_pg_off = PAGE_SIZE - iov->iov_len;
+			}
+		}
+	}
+}
+
+static __noinline void
+sbtls_encrypt(struct mbuf_ext_pgs *pgs)
+{
+	uint64_t seqno;
+	struct sbtls_info *tls;
+	struct socket *so;
+	struct mbuf *top, *m;
+	vm_paddr_t parray[1+ btoc(TLS_MAX_MSG_SIZE_V10_2)];
+	struct iovec src_iov[1 + btoc(TLS_MAX_MSG_SIZE_V10_2)];
+	struct iovec dst_iov[1 + btoc(TLS_MAX_MSG_SIZE_V10_2)];
+	vm_page_t pg;
+	int off, len, npages, page_count, error, i, wire_adj;
+	bool is_anon;
+	bool boring = false;
+
+	so = pgs->so;
+	top = pgs->mbuf;
+	if (so == NULL) {
+		panic("so = NULL, top = %p, pgs = %p\n",
+		    top, pgs);
+	}
+	pgs->so = NULL;
+	pgs->mbuf = NULL;
+	tls = so->so_snd.sb_tls_info;
+	npages = 0;
+	boring = (tls->t_type == SBTLS_T_TYPE_BSSL);
+	/*
+	 *  each TLS record is in a single mbuf.  Do
+	 *  one at a time
+	 */
+	page_count = pgs->enc_cnt;
+	for (m = top; m != NULL && npages != page_count; m = m->m_next) {
+		pgs = (void *)m->m_ext.ext_buf;
+
+
+		KASSERT(((m->m_flags & (M_NOMAP | M_NOTREADY)) ==
+			(M_NOMAP | M_NOTREADY)),
+		    ("%p not unready & nomap mbuf (top = %p)\n", m, top));
+
+		/*
+		 * If this is not a file-backed page, it can
+		 * be used for in-place encryption.
+		 */
+		is_anon = M_WRITABLE(m);
+
+		off = pgs->first_pg_off;
+		seqno = pgs->seqno;
+		wire_adj = 0;
+		for (i = 0; i < pgs->npgs; i++, off = 0) {
+			len = mbuf_ext_pg_len(pgs, i, off);
+			src_iov[i].iov_len = len;
+			src_iov[i].iov_base =
+			    (char *)(void *)PHYS_TO_DMAP(pgs->pa[i]) + off;
+
+			if (is_anon) {
+				dst_iov[i].iov_base = src_iov[i].iov_base;
+				dst_iov[i].iov_len = src_iov[i].iov_len;
+				continue;
+			}
+			/* allocate pages needed for encryption */
+retry_page:
+			pg = vm_page_alloc(NULL, 0, VM_ALLOC_SYSTEM |
+			    VM_ALLOC_NOOBJ | VM_ALLOC_NODUMP);
+			if (pg == NULL) {
+				if (wire_adj)
+					vm_wire_add(wire_adj);
+				wire_adj = 0;
+				vm_wait(NULL);
+				goto retry_page;
+			}
+			wire_adj++;
+			parray[i] = VM_PAGE_TO_PHYS(pg);
+			dst_iov[i].iov_base =
+			    (char *)(void *)PHYS_TO_DMAP(parray[i]) + off;
+			dst_iov[i].iov_len = len;
+		}
+
+		npages += i;
+		if (wire_adj)
+			vm_wire_add(wire_adj);
+
+		error = (*tls->sb_tls_crypt)(tls,
+		    (struct tls_record_layer *)pgs->hdr,
+		    pgs->trail, src_iov, dst_iov, i, seqno);
+		if (error) {
+			/* WTF can we do..? */
+			counter_u64_add(sbtls_offload_failed_crypto, 1);
+			so->so_proto->pr_usrreqs->pru_abort(so);
+			so->so_error = EIO;
+			mb_free_notready(top, page_count);
+			goto drop;
+		}
+		if (!is_anon) {
+			/* Free the old pages that backed the mbuf */
+			m->m_ext.ext_free(m);
+
+			/* Replace them with the new pages just alloc'ed */
+			for (i = 0; i < pgs->npgs; i++)
+				pgs->pa[i] = parray[i];
+
+			/*
+			 * Switch the free routine to basic one.
+			 */
+			m->m_ext.ext_free = mb_free_mext_pgs;
+		}
+		if (boring) {
+			int pgs_removed = 0;
+			sbtls_boring_fixup(tls, &so->so_snd, m, dst_iov,
+			    &pgs_removed);
+			npages -= pgs_removed;
+			page_count -= pgs_removed;
+		}
+	}
+	CURVNET_SET(so->so_vnet);
+	(void) (*so->so_proto->pr_usrreqs->pru_ready)(so, top, npages);
+	CURVNET_RESTORE();
+drop:
+	SOCK_LOCK(so);
+	sorele(so);
+}
+
+static void
+sbtls_work_thread(void *ctx)
+{
+	struct sbtls_wq *wq = ctx;
+	struct mbuf_ext_pgs *p, *n;
+
+
+	STAILQ_INIT(&wq->head);
+	fpu_kern_thread(0);
+	mtx_init(&wq->mtx, "sbtls work queue lock", "tls_wqlock",
+	    MTX_DEF);
+	mtx_lock(&wq->mtx);
+
+	while (1) {
+		wq->running = 0;
+		mtx_sleep(wq, &wq->mtx, 0, "sbtls wq", 0);
+		wq->running = 1;
+		while (NULL != (p = STAILQ_FIRST(&wq->head))) {
+			/* pull the entire list off */
+			STAILQ_INIT(&wq->head);
+			mtx_unlock(&wq->mtx);
+			/* encrypt each mbuf chain on the list */
+			while (p != NULL) {
+				n = STAILQ_NEXT(p, stailq);
+				STAILQ_NEXT(p, stailq) = NULL;
+				sbtls_encrypt(p);
+				counter_u64_add(sbtls_cnt_on, -1);
+				p = n;
+			}
+			mtx_lock(&wq->mtx);
+		}
+	}
+}
diff --git a/sys/kern/uipc_socket.c b/sys/kern/uipc_socket.c
index d920f0f6e5e67..6e7cef7be7c63 100644
--- a/sys/kern/uipc_socket.c
+++ b/sys/kern/uipc_socket.c
@@ -131,6 +131,7 @@ __FBSDID("$FreeBSD$");
 #include <sys/protosw.h>
 #include <sys/socket.h>
 #include <sys/socketvar.h>
+#include <sys/sockbuf_tls.h>
 #include <sys/resourcevar.h>
 #include <net/route.h>
 #include <sys/signalvar.h>
@@ -142,7 +143,7 @@ __FBSDID("$FreeBSD$");
 #include <sys/jail.h>
 #include <sys/syslog.h>
 #include <netinet/in.h>
-
+#include <netinet/tcp.h>
 #include <net/vnet.h>
 
 #include <security/mac/mac_framework.h>
@@ -1438,6 +1439,18 @@ sosend_generic(struct socket *so, struct sockaddr *addr, struct uio *uio,
 	ssize_t resid;
 	int clen = 0, error, dontroute;
 	int atomic = sosendallatonce(so) || top;
+	struct sbtls_info *tls;
+	int pru_flag, tls_pruflag, tls_enq_cnt;
+	uint8_t tls_rtype = TLS_RLTYPE_APP;
+	bool tls_control = false;
+
+	if ((so->so_snd.sb_tls_flags & SB_TLS_ACTIVE) != 0) {
+		tls = so->so_snd.sb_tls_info;
+		tls_pruflag = PRUS_NOTREADY;
+	}  else {
+		tls = NULL;
+		tls_pruflag = 0;
+	}
 
 	if (uio != NULL)
 		resid = uio->uio_resid;
@@ -1463,9 +1476,20 @@ sosend_generic(struct socket *so, struct sockaddr *addr, struct uio *uio,
 	    (so->so_proto->pr_flags & PR_ATOMIC);
 	if (td != NULL)
 		td->td_ru.ru_msgsnd++;
-	if (control != NULL)
+	if (control != NULL) {
+		struct cmsghdr *cm = mtod(control, struct cmsghdr *);
 		clen = control->m_len;
 
+		if (tls != NULL && clen >= sizeof(*cm) &&
+		    cm->cmsg_type == TLS_SET_RECORD_TYPE) {
+			tls_rtype = *((uint8_t *)CMSG_DATA(cm));
+			clen = 0;
+			m_freem(control);
+			control = NULL;
+			tls_control = true;
+		}
+	}
+
 	error = sblock(&so->so_snd, SBLOCKWAIT(flags));
 	if (error)
 		goto out;
@@ -1518,7 +1542,8 @@ sosend_generic(struct socket *so, struct sockaddr *addr, struct uio *uio,
 			goto release;
 		}
 		if (space < resid + clen &&
-		    (atomic || space < so->so_snd.sb_lowat || space < clen)) {
+		    (atomic || tls_control ||
+			space < so->so_snd.sb_lowat || space < clen)) {
 			if ((so->so_state & SS_NBIO) || (flags & MSG_NBIO)) {
 				SOCKBUF_UNLOCK(&so->so_snd);
 				error = EWOULDBLOCK;
@@ -1546,10 +1571,27 @@ sosend_generic(struct socket *so, struct sockaddr *addr, struct uio *uio,
 				 * is a workaround to prevent protocol send
 				 * methods to panic.
 				 */
-				top = m_uiotombuf(uio, M_WAITOK, space,
-				    (atomic ? max_hdr : 0),
-				    (atomic ? M_PKTHDR : 0) |
-				    ((flags & MSG_EOR) ? M_EOR : 0));
+				if (tls != NULL) {
+					top = m_uiotombuf(uio, M_WAITOK, space,
+					    tls->sb_params.sb_maxlen,
+					    M_NOMAP |
+					    ((flags & MSG_EOR) ? M_EOR : 0));
+					if (top != NULL) {
+						error = sbtls_frame(&top,
+						    tls, &tls_enq_cnt,
+						    tls_rtype);
+						if (error) {
+							m_freem(top);
+							goto release;
+						}
+					}
+					tls_rtype = TLS_RLTYPE_APP;
+				} else {
+					top = m_uiotombuf(uio, M_WAITOK, space,
+					    (atomic ? max_hdr : 0),
+					    (atomic ? M_PKTHDR : 0) |
+					    ((flags & MSG_EOR) ? M_EOR : 0));
+				}
 				if (top == NULL) {
 					error = EFAULT; /* only possible error */
 					goto release;
@@ -1573,8 +1615,8 @@ sosend_generic(struct socket *so, struct sockaddr *addr, struct uio *uio,
 			 * this.
 			 */
 			VNET_SO_ASSERT(so);
-			error = (*so->so_proto->pr_usrreqs->pru_send)(so,
-			    (flags & MSG_OOB) ? PRUS_OOB :
+
+			pru_flag = (flags & MSG_OOB) ? PRUS_OOB :
 			/*
 			 * If the user set MSG_EOF, the protocol understands
 			 * this flag and nothing left to send then use
@@ -1586,13 +1628,35 @@ sosend_generic(struct socket *so, struct sockaddr *addr, struct uio *uio,
 				PRUS_EOF :
 			/* If there is more to send set PRUS_MORETOCOME. */
 			    (flags & MSG_MORETOCOME) ||
-			    (resid > 0 && space > 0) ? PRUS_MORETOCOME : 0,
-			    top, addr, control, td);
+			    (resid > 0 && space > 0) ? PRUS_MORETOCOME : 0;
+
+			pru_flag |= tls_pruflag;
+
+			if (tls != NULL)
+				soref(so);
+
+			error = (*so->so_proto->pr_usrreqs->pru_send)(so,
+			    pru_flag, top, addr, control, td);
+
 			if (dontroute) {
 				SOCK_LOCK(so);
 				so->so_options &= ~SO_DONTROUTE;
 				SOCK_UNLOCK(so);
 			}
+
+			if (tls != NULL) {
+				/*
+				 * Note that error is intentionally
+				 * ignored.
+				 *
+				 * Like sendfile(), we rely on the
+				 * completion routine (pru_ready())
+				 * to free the mbufs in the event that
+				 * pru_send() encountered an error and
+				 * did not append them to the sockbuf.
+				 */
+				sbtls_enqueue(top, so, tls_enq_cnt);
+			}
 			clen = 0;
 			control = NULL;
 			top = NULL;
@@ -1977,7 +2041,11 @@ soreceive_generic(struct socket *so, struct sockaddr **psa, struct uio *uio,
 			SBLASTRECORDCHK(&so->so_rcv);
 			SBLASTMBUFCHK(&so->so_rcv);
 			SOCKBUF_UNLOCK(&so->so_rcv);
-			error = uiomove(mtod(m, char *) + moff, (int)len, uio);
+			if ((m->m_flags & M_NOMAP) != 0)
+				error = m_unmappedtouio(m, moff, uio, (int)len);
+			else
+				error = uiomove(mtod(m, char *) + moff,
+				    (int)len, uio);
 			SOCKBUF_LOCK(&so->so_rcv);
 			if (error) {
 				/*
diff --git a/sys/netinet/tcp.h b/sys/netinet/tcp.h
index 97ef1f5738eb6..df31cf913b8cc 100644
--- a/sys/netinet/tcp.h
+++ b/sys/netinet/tcp.h
@@ -173,6 +173,7 @@ struct tcphdr {
 #define	TCP_KEEPINTVL	512	/* L,N interval between keepalives */
 #define	TCP_KEEPCNT	1024	/* L,N number of keepalives before close */
 #define	TCP_FASTOPEN	1025	/* enable TFO / was created via TFO */
+#define	TCP_TLS_ENABLE	1026	/* TLS Crypt enable */
 #define	TCP_PCAP_OUT	2048	/* number of output packets to keep */
 #define	TCP_PCAP_IN	4096	/* number of input packets to keep */
 #define TCP_FUNCTION_BLK 8192	/* Set the tcp function pointers to the specified stack */
@@ -259,5 +260,8 @@ struct tcp_function_set {
 	char function_set_name[TCP_FUNCTION_NAME_LEN_MAX];
 	uint32_t pcbcnt;
 };
-
+/*
+ * TCP Control message types
+ */
+#define TLS_SET_RECORD_TYPE 1
 #endif /* !_NETINET_TCP_H_ */
diff --git a/sys/netinet/tcp_usrreq.c b/sys/netinet/tcp_usrreq.c
index f4bf0d9cf2e8d..0cc55d51941f8 100644
--- a/sys/netinet/tcp_usrreq.c
+++ b/sys/netinet/tcp_usrreq.c
@@ -59,6 +59,7 @@ __FBSDID("$FreeBSD$");
 #endif /* INET6 */
 #include <sys/socket.h>
 #include <sys/socketvar.h>
+#include <sys/sockbuf_tls.h>
 #include <sys/protosw.h>
 #include <sys/proc.h>
 #include <sys/jail.h>
@@ -1531,6 +1532,7 @@ tcp_default_ctloutput(struct socket *so, struct sockopt *sopt, struct inpcb *inp
 	int	error, opt, optval;
 	u_int	ui;
 	struct	tcp_info ti;
+	struct  tls_so_enable tls;
 	struct cc_algo *algo;
 	char	*pbuf, buf[TCP_CA_NAME_MAX];
 	size_t	len;
@@ -1690,6 +1692,16 @@ tcp_default_ctloutput(struct socket *so, struct sockopt *sopt, struct inpcb *inp
 			INP_WUNLOCK(inp);
 			break;
 
+		case TCP_TLS_ENABLE:
+			INP_WUNLOCK(inp);
+			error = sooptcopyin(sopt, &tls, sizeof(tls),
+			    sizeof(tls));
+			INP_WLOCK_RECHECK(inp);
+			if (!error)
+				error = sbtls_crypt_tls_enable(so, &tls);
+			INP_WUNLOCK(inp);
+			break;
+
 		case TCP_KEEPIDLE:
 		case TCP_KEEPINTVL:
 		case TCP_KEEPINIT:
diff --git a/sys/sys/sockbuf.h b/sys/sys/sockbuf.h
index 3d2612f204213..3b1f836481288 100644
--- a/sys/sys/sockbuf.h
+++ b/sys/sys/sockbuf.h
@@ -68,6 +68,7 @@ struct sockaddr;
 struct socket;
 struct thread;
 struct selinfo;
+struct sbtls_info;
 
 /*
  * Variables for socket buffering.
@@ -98,6 +99,9 @@ struct	sockbuf {
 	u_int	sb_ctl;		/* (a) non-data chars in buffer */
 	int	sb_lowat;	/* (a) low water mark */
 	sbintime_t	sb_timeo;	/* (a) timeout for read/write */
+	uint64_t sb_tls_seqno;	/* TLS seqno */
+	struct	sbtls_info *sb_tls_info; /* TLS state */
+	u_int	sb_tls_flags;	/* flags used by TLS */
 	short	sb_flags;	/* (a) flags, see below */
 	int	(*sb_upcall)(struct socket *, void *, int); /* (a) */
 	void	*sb_upcallarg;	/* (a) */
diff --git a/sys/sys/sockbuf_tls.h b/sys/sys/sockbuf_tls.h
new file mode 100644
index 0000000000000..5009fd38ae544
--- /dev/null
+++ b/sys/sys/sockbuf_tls.h
@@ -0,0 +1,249 @@
+/*-
+ * Copyright (c) 2014
+ *	Netflix Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+#ifndef _SYS_SOCKBUF_TLS_H_
+#define _SYS_SOCKBUF_TLS_H_
+#include <sys/types.h>
+
+struct tls_record_layer {
+	uint8_t  tls_type;
+	uint8_t  tls_vmajor;
+	uint8_t  tls_vminor;
+	uint16_t tls_length;	
+	uint8_t  tls_data[0];
+} __attribute__ ((packed));
+
+#define TLS_MAX_MSG_SIZE_V10_2	16384
+#define TLS_MAX_PARAM_SIZE	1024	/* Max key/mac/iv in sockopt */
+#define TLS_AEAD_GCM_LEN	4
+
+/* Type values for the record layer */
+#define TLS_RLTYPE_APP		23
+
+/*
+ * Constants for the Socket Buffer TLS state flags (sb_tls_flags).
+ */
+#define	SB_TLS_ACTIVE		0x0001	/* set if SO_CRYPT_TLS is enabled. */
+#define	SB_TLS_CRY_INI		0x0002  /* state being initialized */
+#define	SB_TLS_RECV_SIDE	0x0004	/* rx (decrypt/decapsulate) */
+#define	SB_TLS_SEND_SIDE	0x0008	/* tx (encrypt/encapsulate) */
+
+/*
+ * Alert protoocol
+ */
+struct tls_alert_protocol {
+	uint8_t	level;
+	uint8_t desc;
+} __attribute__ ((packed)); 
+
+/*
+ * AEAD nonce for GCM data.
+ */
+struct tls_nonce_data {
+	uint8_t fixed[TLS_AEAD_GCM_LEN];
+	uint64_t seq;
+} __attribute__ ((packed)); 
+
+/*
+ * AEAD added data format per RFC.
+ */
+struct tls_aead_data {
+	uint64_t seq;	/* In network order */
+	uint8_t type;
+	uint8_t  tls_vmajor;
+	uint8_t  tls_vminor;
+	uint16_t tls_length;	
+} __attribute__ ((packed));
+
+/*
+ * Stream Cipher MAC input not sent on wire
+ * but put into the MAC.
+ */
+struct tls_mac_data {
+	uint64_t seq;
+	uint8_t type;
+	uint8_t  tls_vmajor;
+	uint8_t  tls_vminor;
+	uint16_t tls_length;	
+} __attribute__ ((packed));
+
+/* Not used but here is the layout
+ * of what is on the wire for
+ * a TLS record that is a stream cipher.
+ *
+struct tls_ss_format {
+	uint8_t IV[record_iv_len]; TLS pre 1.1 this is missing.
+	uint8_t content[len];
+	uint8_t MAC[maclen];
+	uint8_t padding[padlen];
+	uint8_t padlen;
+};
+*
+* We don't support in-kernel pre-1.1 TLS so if the
+* user requests that, we error during SO_TLS_ENABLE.
+* Each pad byte in padding must contain the same value
+* as padlen. Also note that content <-> padlen should
+* be mod 0 to the blocklen of the cipher. I am guessing
+* the IV is a length of the multiple of the cipher as
+* well.
+*/
+
+#define TLS_MAJOR_VER_ONE	3
+#define TLS_MINOR_VER_ZERO	1	/* 3, 1 */
+#define TLS_MINOR_VER_ONE	2	/* 3, 2 */
+#define TLS_MINOR_VER_TWO	3	/* 3, 3 */
+
+struct sockopt;
+struct uio;
+
+/* For TCP_TLS_ENABLE */
+struct tls_so_enable {
+	const uint8_t *hmac_key;
+	const uint8_t *crypt;
+	const uint8_t *iv;
+	uint32_t crypt_algorithm; /* e.g. CRYPTO_AES_CBC */
+	uint32_t mac_algorthim;	  /* e.g. CRYPTO_SHA2_256_HMAC */
+	uint32_t key_size;	  /* Length of the key */
+	int hmac_key_len;
+	int crypt_key_len;
+	int iv_len;
+	uint8_t tls_vmajor;
+	uint8_t tls_vminor;
+};
+
+struct tls_kern_params {
+	uint8_t *hmac_key;
+	uint8_t *crypt;
+	uint8_t *iv;
+	uint16_t hmac_key_len;
+	uint16_t crypt_key_len;
+	uint16_t iv_len;
+	uint16_t sb_maxlen;
+	uint8_t sb_tls_vmajor;
+	uint8_t sb_tls_vminor;
+	uint8_t sb_tls_hlen;
+	uint8_t sb_tls_tlen;
+	uint8_t sb_tls_bs;
+};
+
+#define SBTLS_T_TYPE_OCFW		1	/* Open Crypto Framework */
+#define SBTLS_T_TYPE_BSSL		2	/* Boring SSL */
+#define SBTLS_T_TYPE_INTELISA_GCM	3	/* Intel ISA AES GCM */
+
+#define SBTLS_INTELISA_AEAD_TAGLEN	16
+#define SBTLS_INTELISA_CBC_TAGLEN	16
+#ifdef _KERNEL
+
+#include <sys/malloc.h>
+
+MALLOC_DECLARE(M_TLSSOBUF);
+
+#define SBTLS_API_VERSION 4
+
+struct sbtls_info;
+struct iovec;
+
+struct sbtls_crypto_backend {
+	LIST_ENTRY(sbtls_crypto_backend) next;
+	void (*setup_cipher) (struct sbtls_info *tls, int *err);
+	int (*try) (struct socket *so,
+	    struct tls_so_enable *en, int *error);
+	void (*clean_cipher) (struct sbtls_info *tls, void *cipher);
+	int prio;
+	int api_version;
+	int use_count;                  /* dev testing */
+	const char *name;
+};
+
+struct sbtls_info {
+	int	(*sb_tls_crypt)(struct sbtls_info *tls,
+	    struct tls_record_layer *hdr, uint8_t *trailer,
+	    struct iovec *src, struct iovec *dst, int iovcnt,
+	    uint64_t seqno);
+	void *cipher;
+	struct tls_kern_params sb_params;
+	uint16_t sb_tsk_instance;	/* For task selection */
+	struct sbtls_crypto_backend *be;/* backend crypto impl. */
+	uint8_t t_type; 	 	/* Flags indicating type of encode */
+	uint8_t taglen;                 /* for CBC tag padding */
+} __aligned(CACHE_LINE_SIZE);
+
+
+#ifndef KERN_TLS
+#include "opt_kern_tls.h"
+#endif
+
+#ifndef KERN_TLS
+
+/* TLS stubs so we can compile kernels without options KERN_TLS */
+
+static inline int
+sbtls_crypt_tls_enable(struct socket *so,
+    struct tls_so_enable *en)
+{
+	return (ENOTSUP);
+}
+
+static inline void
+sbtlsdestroy(struct sockbuf *sb)
+{
+}
+
+static inline int
+sbtls_frame(struct mbuf **m, struct sbtls_info *tls, int *enqueue_cnt,
+    uint8_t record_type)
+{
+	return (ENOTSUP);
+}
+
+static inline void
+sbtls_enqueue(struct mbuf *m, struct socket *so, int page_count)
+{
+}
+
+static inline void
+sbtls_seq(struct sockbuf *sb, struct mbuf *m)
+{
+}
+#else
+
+int sbtls_crypto_backend_register(struct sbtls_crypto_backend *be);
+int sbtls_crypto_backend_deregister(struct sbtls_crypto_backend *orig_be);
+int sbtls_crypt_tls_enable(struct socket *so, struct tls_so_enable *en);
+void sbtlsdestroy(struct sockbuf *sb);
+struct sbtls_info *sbtls_init_sb_tls(struct socket *so,
+    struct tls_so_enable *en, size_t cipher_len);
+void sbtls_free_tls(struct sbtls_info *tls);
+int sbtls_frame(struct mbuf **m, struct sbtls_info *tls, int *enqueue_cnt,
+    uint8_t record_type);
+void sbtls_seq(struct sockbuf *sb, struct mbuf *m);
+void sbtls_enqueue(struct mbuf *m, struct socket *so, int page_count);
+
+
+#endif /* KERN_TLS */
+#endif /* _KERNEL */
+#endif /* _SYS_SOCKBUF_TLS_H_ */
